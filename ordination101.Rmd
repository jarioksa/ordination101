---
title: "Basic Ordination Methods"
subtitle: "Ordination 101 for Ecological Methods"
author: "Jari Oksanen"
date: "Nov 8 to 15, 2017"
output:
  xaringan::moon_reader:
    css: ['default', './resources/gavins.css']
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(comment=NA,
                      echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE,
                      fig.align = 'center', fig.height = 5, fig.width = 5.5, dev = 'svg')
knitr::knit_hooks$set(crop.plot = knitr::hook_pdfcrop)
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(rgl.newwindow = TRUE)
```
```{r packages, include = FALSE, cache = FALSE}
require('vegan')
if (packageVersion('vegan') < '2.5.0')
   stop("needs vegan 2.5-0")
data('varespec', 'varechem', 'mite', 'mite.env', 'dune', 'dune.env')
data('dune.phylodis','dune.taxon')
require('natto') # in github, natto::raodist()
require('labdsv')
data('bryceveg')
require(analogue)
data(abernethy)
library('mgcv')
require('knitr')
require('viridis')
```
<!-- ********** INTRO ************ -->
<!-- ********** PCA ************** -->
---
class: inverse center middle

# PCA: Principal Components Analysis

PCA  is the  basic ordination  method and  the one  that Statisticians
know. It is poor for most uses,  but we need to know PCA to understand
how other methods improve upon it.  The main problem of PCA is that it
is  linear, but  community  data are  nonlinear.
---
# From Regression to Ordination

- What is the best possible predictor variable for a species? --- It
  is the species itself!

- Some species can also be strong in predicting the abundances of
  other species

- The best possible predictor is such a (linear) combination of
  species that explains abundances of all species as well as possible

- That linear combination is the first PCA axis
---
# 3 Cryptogams in Reindeer Pastures

```{r bottpredict, fig.width=8, fig.height=5.5}
bott <- varespec[, c("Pleuschr","Cladrang","Cladstel")]
bott0 <- scale(bott, scale = FALSE)
par(mar=c(4,4,1,1)+.1)
palette(viridis(8))
panel.lm <- function(x, y, col.smooth = 1, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    points(x, y, ...)
    mod <- lm(y ~ x)
    txt <- paste0("Resid.Var = ", round(summary(mod)$sigma^2, 1))
    abline(mod, col = col.smooth, lwd=2)
    text(mean(usr[1:2]), 0.9*usr[4], txt)
}
pairs(bott0, pch=16, col = 4, ylim = range(bott0), panel = panel.lm)
```
---
# Best Predictor
```{r bottpc1, fig.width=10}
mod <- rda(bott0)
koef <- mod$CA$v[,1]
xlab = paste(formatC(koef,digits=2, flag="+"), names(koef), sep="*",collapse="")
PC1 <- (bott0 %*% mod$CA$v)[,1]
df <- data.frame(bott0)
palette(viridis(8))
par(mar=c(4,4,1,1)+.1, mfrow=c(1,3))
ylim <- range(bott0)
plot(Pleuschr ~ PC1, df, type="n", ylim=ylim, xlab=xlab)
panel.lm(PC1, df$Pleuschr, pch=16, col = 2, col.smooth=4)
plot(Cladrang ~ PC1, df, type="n", ylim=ylim, xlab=xlab)
panel.lm(PC1, df$Cladrang, pch=16, col = 2, col.smooth=4)
plot(Cladstel ~ PC1, df, type="n", ylim=ylim, xlab=xlab)
panel.lm(PC1, df$Cladstel, pch=16, col = 2, col.smooth=4)

```
---
# What Was Explained?

- PCA explains the **variance** of the data: the squared
  differences between observed values $y$ and species means $\bar y$:
  $\mathrm{Var}(y_j) = \sum_{i=1}^N(y_{ij} - \bar y_j)^2/(N-1)$
  - NB., ordination always uses centred data so that $\bar y = 0$

- In the example, the variances of three species were
  `r round(apply(bott0, 2, var), 1)`, with sum
  `r round(mod$tot.chi, 1)`, and the sum of residual variances of the PC
  regression was `r round(mod$tot.chi - mod$CA$eig[1], 1)`

- The variance explained by the first PC is the **eigenvalue** of the
  axis, and it is the difference of total variance and residual
  variance $\lambda_1 =$ `r round(mod$CA$eig[1], 1)` which is
  `r round(100*mod$CA$eig[1]/mod$tot.chi, 1)`% of total variance

- The coefficients (`r round(mod$CA$v[,1], 3)`) of three species are
  the **species scores** used in ordination diagrams, and they are also
  regression coefficients that project centred species abundances to
  the PC
  - NB., ordination always uses centred data so that intercept
  is zero

- The result of this projection are the sampling unit **(SU) scores**
  that are used in ordination diagrams -- this is also called as the
  **principal component**

---
# Second and Further Principal Components

- Second PC has similar components, but is **orthogonal** to the
  previous one:

- Species scores and SU scores are orthogonal to (uncorrelated with)
  correspoding scores in the previous axes

- The second PC explains the largest possible amount of remaining
  variance
  - In the example, its eigenvalue is $\lambda_2 =$ `r round(mod$CA$eig[2], 1)`
    which is  `r round(100*mod$CA$eig[2]/mod$tot.chi, 1)`% of total variance

- The sum of all $R$ eigenvalues is equal to the variances of all
   species $\sum_{r=1}^R \lambda_r = \sum_{j=1}^S \mathrm{Var}(y_j)$:
   The axes decompose variance into independent ordered components

- In modern software, we do not find second and further PCs one-by-one
  with orthogonalization, but the interpretation of the results is
  still similar

---
# Is This Circular?

- We explain species by species &mdash; and this sound badly circular

- We perform no statistical testing, and what looks regression
  actually is **rotation** of species data

- We only like to find a way of looking at the data so that some first
  (say PC1 and PC2) so as much of the variance of the data as possible

- Then we can ignore the later axes and say that a **major part** of
  variation in the data is displayed in the first dimensions that we
  can graph

- PCA is **not** a statistical method, but it is only a rotation of
  the data

---
# Species Scores in Multidimensions

.pull-left[

- We had species scores for a single axis, but in 2 and more dimensions
  we must look at all dimensions simultaneously

- The coefficients together define the **direction** to which the
  species abundance **increases** most steeply &mdash; and it
  **decreases** in the opposite direction

- The response is linear in 2D: the contours of linear trend surface are
  perpendicular to the arrow

- The estimated abundances of all species on each SU can be read by
  projecting the SU point to the arrow: PCA **approximates** data

- Species were centred: The zero contour goes through the origin

]

.pull-right[
```{r plot3sp}
par(mar=c(4,4,1,1)+.1)
palette(viridis(8))
biplot(mod, type = c("text","points"))
tmp <- ordisurf(mod ~ Cladstel, df, add = TRUE, col=4, knots=1)
```
]

---
# Distance and Direction From the Origin

.pull-left[

- In the origin, each species occurs at its average abundance $\bar y_j$

- The further away from the origin SU is situated, the more strongly
  species abundances differ from the average

- For species the angles between arrows indicate similar pattern of
  change, and the length of the arrow the steepness of the change

- Try yourself: where these species are most abundant and scarcest?
  Which species attain highest abundances and where? Which species
  replace each other and which are independent from each other?
]

.pull-right[

```{r plot3sp2}
par(mar=c(4,4,1,1)+.1)
palette(viridis(8))
biplot(mod, col=c(4,1))
```

]
---
# Rotation in Species Space

### Please Add Me!
---
# Complete Reindeer Pasture Data

.pull-left[

- Analysis of complete data not too different from just three species

- PCA analyses variances and shows absolute differences from the mean
  $y - \bar y$: minor species change little in absolute units

- Actually only some few species influence results, and the data are
  not very multivariate

]

.pull-right[
```{r pcavare}
par(mar=c(4,4,1,1)+.1)
palette(viridis(8))
modf <- rda(varespec)
biplot(modf, col=c(4,1))
```
]

---
# Explaining Variance

.pull-left[

- Abundant species have high variance, and PCA explains variance

- It may be very easy to explain variance, when only a few species
  account for the most, because then data are not very
  multidimensional

- Minor species may be ecological indicators, but they have little
  effect on community ordination with PCA

- Three abundant cryptogams accounted for
  `r round(100*mod$tot.chi/modf$tot.chi, 1)`% of total variance in the complete
  data set, and are dominant in PCA

]

.pull-right[

```{r varabu}
par(mar=c(4,4,1,1)+.1)
v <- apply(varespec, 2, var)
plot(colMeans(varespec), v, xlab="Mean Abundance", ylab="Variance", type="n")
ordilabel(cbind(colMeans(varespec), v), priority = v, fill = "yellow")
```

]
---
# Equal Weights to All Species

.pull-left[

- Use correlation instead of variance and minimize residual $1-R^2$:
  This gives equal weights to all species

- Equalized data are more multidimensional, and scarce species will
  also influence the result, and they change from the previous

- PCA implies that species effects should be shown by arrows, but this
  makes graphs very messy, and often it makes sense to drop arrows and
  just show the species scores at arrowheads

- They still should be interpreted like arrows: Distance and direction
  from the origin counts

]

.pull-right[
```{r pcacorr}
mod1 <- rda(varespec, scale=TRUE)
palette(viridis(8))
par(mar=c(4,4,1,1)+.1)
plot(mod1, scaling="sites", type="n")
text(mod1, scaling="sites", dis="site", col = 1, cex=0.7)
text(mod1, scaling="sites", dis="species", col=4, cex=0.8)
```
]
---
class:  inverse center middle

## PCA Trouble: It is Linear!
---
# Linear Method

- PCA builds upon idea of linear regression

- PC axis are sometimes called as latent variables: unknown and hidden
  real variables that we dig up from data

- Community response to environmental variables is non-linear: PC axes
  cannot find such variables

```{r mtfgrad, fig.width=10, fig.height=3}
source('data/mtfit.R')
palette(viridis(6))
par(mar=c(4,4,1,1)+.1)
matplot(altfit, mtfit, type="l", lty=1, xlab="Altitude (m)", ylab="Species Response")
```
---
# Horseshoe: Beware!

.pull-left[

- If species have nonlinear, unimodal response to gradients, the
  gradient appears as a curve: **horseshoe artifact**

- People often want to interpret axes as latent variables, but even
  single gradient is not an axis: **Beware**

- Even if you are aware of the risk of horseshoe, it can hide other
  shorter gradients: using two dimensions for one gradient is wasteful
  and confusing

- In general, PCA **should not be used** for community data

- PCA is useful for linear data, or for deriving new combined
  variables for other analyses when PCA explains a large proportion of
  variance

]

.pull-right[
```{r horseshoe}
mod <- rda(mtfit)
palette(viridis(8))
par(mar=c(4,4,1,1)+.1)
plot(mod, dis="si", scaling="si", type="n")
points(mod, dis="si", scaling="si", pch=16)
```
]
---
# Horseshoe is Common

```{r morehorses, fig.width=10}
palette(viridis(8))
par(mfrow=c(1,3), mar=c(4,4,2,1)+.1)
m1 <- rda(varespec)
m2 <- rda(bryceveg)
m3 <- rda(mite)
plot(m1, dis="si", type="n", main="Reindeer Pastures")
points(m1, dis="si", pch=16)
plot(m2, dis="si", type="n", main="Bryce Canyon")
points(m2, dis="si", pch=16)
plot(m3, dis="si", type="n", main="Oribatid Mites")
points(m3, dis="si", pch=16)
```
---
# When Would We Expect a Horseshoe?


- **Always** with community data

- Even when we cannot see the horseshoe, it may be hidden below the
  crumble of noisy analysis

- Horseshoe can hide underlying gradients and disperse their effect
  over several PCs so that they cannot be found and interpreted

---
# Niche for PCA

.pull-left[

- New synthetic variables from correlated measurements to avoid
  problems of collinearity

- For this PCA should explain a large amount of variance

- Use only the subset of correlated variables that may be assumed to
  descrige the same underlying (latent) variable: **measurement
  model**

- Use correlations when variables are not measured in the same units

- Do not use axes directly: they may not be parallel to interpretable
  variables

]

.pull-right[

```{r latent}
mod <- rda(varechem, scale=TRUE)
palette(viridis(8))
par(mar=c(4,4,1,1)+.1)
biplot(mod, display="sp", col=1)
```

]

<!-- ********* CA ************ -->
---
class: inverse center middle

# CA: Correspondence Analysis

Correspondence Analysis is an eigenvector method just like PCA, and it
can be seen as its variant. However, this small difference makes it
handle nonlinear species responses better than PCA. For community
data, CA is usually a much better alternative than PCA.

---
# Correspondence Analysis

- In PCA we explain the differences of species from their average
  $y_{ij} - \bar y_j$, using its mean square (variance) as a measure
  of goodness of fit

- In CA the expected value is derived both from the SU and species,
  and the goodness of fit is $\chi^2$

- We first scale the observed data so that their sum is one
  $\left( \sum_{i=1}^N \sum_{j=1}^S y_{ij} = 1 \right)$
  &mdash; that is, we divide data with its total

- If all communities have identical species composition, and all
  species occur in equal proportion in all SUs, then the expected
  abundance is $r_i c_j$ where $r_i$ is row (SU) sum and $c_j$ is the
  column (species) sum: This takes the place of $\bar y_j$ of PCA

- We look at proportional difference
  $(y_{ij} - r_i c_j)/\sqrt{r_i c_j}$, and its square is
  $\chi^2 = (y_{ij} - r_i c_j)^2/(r_i c_j)$:
  This takes the place of variance of PCA

- Then we proceed with regression to find the axes, just like in PCA
  (except that we need to use weights $\sqrt{r_i}$, $\sqrt{c_j}$)

---
# Small but Important Difference

.pull-left[

- CA is based on similar regression as PCA, but with
  $\chi$-standardized data and weights

- Regression is linear, but *observed* values of data rather pack so
  that high values are close to each other along the axis

- The response of species can be approximately unimodal: this matches
  the gradient model of species responses

- The solution can be better related to environmental variables than in PCA

- CA is still **not** a completely unimodal ordination and it still
  shows a curve artefact, but not as garbled as PCA

]

.pull-right[

```{r cagam}
palette(viridis(3))
par(mar=c(4,4,1,1)+.1)
mod <- cca(bott) # from PCA lecture
CA1 <- mod$CA$u[,1]
matplot(CA1, bott, pch=16, xlab = "CA axis 1", ylab="Species Abundance")
legend("top", colnames(bott), pch=16, lty=1, col=1:3, bty="n")
i <- order(CA1)
for(k in 1:3) {
   m <- gam(bott[,k] ~ s(CA1, k=4), family=quasipoisson)
   lines(CA1[i], fitted(m)[i], lwd=2, col = k)
}
```
]


---
# PCA Orders Linearly, CA Packs

.pull-left[
```{r pcatabasco}
tabasco(varespec, rda(varespec), scale="log", col=viridis(10))
```
**PCA**
]

.pull-right[
```{r catabasco, }
tabasco(varespec, cca(varespec), scale="log", col=viridis(10))
```
**CA**
]

---
# More Clearly in Sparse Data

.pull-left[
```{r pcatabadune}
tabasco(dune, rda(dune), col=viridis(5))
```
**PCA:** Dutch Dune Meadows
]

.pull-right[
```{r catabadune, }
tabasco(dune, cca(dune), col=viridis(5))
```
**CA:** Diagonally structured table
]
---
# CA Preserves Order

- CA can also have a curve artefact with dominant long gradient, but
  it is not garbled and involuted like the PCA horseshoe

- First CA axis preserves the correct ordering: CA is a **seriation method** 

```{r abernethy, fig.width=8, fig.height=3.5}
par(mar=c(4,4,1,1)+.1, mfrow=c(1,2))
palette(viridis(8))
m0 <- rda(abernethy[,1:36])
m1 <- cca(abernethy[,1:36])
plot(m0, dis="si", scaling="si", type="n")
lines(scores(m0, dis="si", scaling="si"), col=7)
points(m0, dis="si", scaling="si", pch=16, col=1)
plot(m1, dis="si", scaling="si", type="n")
lines(scores(m1, dis="si", scaling="si"), col=7)
points(m1, dis="si", scaling="si", pch=16, col=1)
```
.pull-right[
Abernethy Forest pollen data (5515 to 12145 BP)
]
---
# Unimodal Responses in Multidimensions

.pull-left[
- CA packs species also in multidimensional space

- Instead of an arrow of increase, the species score may be regarded
  as a centre of abundance

- It is said that CA approximates the unimodal model

- We may think that the species scores gives the species maximum and
  the abundance decreases to every direction from the centroid given
  by the species score

- In PCA species close to the origin changed little and was poorly
  presented by the ordination, but in CA it really may have its
  optimum there

]

.pull-right[
```{r cascores, fig.height=5.5}
par(mar=c(4,4,1.4,1)+.1, mfrow=c(2,2))
palette(viridis(8))
mod <- cca(dune)
with(dune, tmp <- ordisurf(mod ~ Poaprat, bubble=2, family=quasipoisson, knots=2, col=6, scaling="si", main="Poa pratensis"))
with(dune, tmp <- ordisurf(mod ~ Trifrepe, bubble=2, family=quasipoisson, knots=2, col=6, scaling="si", main="Trifolium repens"))
with(dune, tmp <- ordisurf(mod ~ Lolipere, bubble=2, family=quasipoisson, knots=2, col=6, scaling="si",main="Lolium perenne"))
with(dune, tmp <- ordisurf(mod ~ Juncarti, bubble=2, family=quasipoisson, knots=2, col=6, scaling="si", main="Juncus articulatus"))
```
]

---
# Numbers and Interpretation

.pull-left[

- Sum of all eigenvalues $\sum_k \lambda_k = \chi^2$, and
  $\lambda_k / \chi^2$ gives the proportion of inertia explained by the axis

- This inertia is **not** variance but $\chi^2$, and eigenvalues or percentages
  cannot be compared to PCA

- High eigenvalue is good, but too high (say, $\lambda > 0.7$) are
  suspicious: disjunct or partly disjunct data where a group of SUs
  shares hardly anything (or nothing) with others

- Species scores can be seen as points indication species optimum
  instead of arrows of increase in PCA

- Both species and SU scores are **weighted averages** of each other
  &mdash; but exact relationship depends on scaling in graphs

]

.pull-right[

```{r caplot}
palette(viridis(8))
par(mar=c(4,4,1,1)+.1)
plot(mod, scaling="sites", type="n")
text(mod, scaling="sites", dis="site", col = 1, cex=0.7)
text(mod, scaling="sites", dis="species", col=4, cex=0.8)
```

]

---
# Weighted Averages and Packing

.pull-left[
```{r caspecwa}
par(mar=c(4,4,1,1)+.1)
palette(viridis(8))
with(dune, tmp <- ordisurf(mod ~ Lolipere, bubble=2.5, family=quasipoisson, knots=2, main="scaling='species'", pch=21, col=1, bg=4))
text(mod, dis="sp", cex=0.6, col=3)
with(dune, ordispider(mod, Lolipere>0, w=Lolipere, label=FALSE, show=TRUE, col=4))
 with(dune, plot(envfit(mod ~ rep("A",nrow(dune)), w=Lolipere), labels="Lolipere", bg=8))
```
**Dots:** SUs, *size:* abundance of *Lolium perenne*, **Text:** species
]

.pull-right[
```{r casitewa}
par(mar=c(4,4,1,1)+.1)
palette(viridis(8))
plot(mod, scaling="si", type="n", main="scaling='sites'")
ordispider(mod, unlist(dune[18,]>0), scaling="si", w = unlist(dune[18,]), show=T, display="sp", col=4)
cl <- rep("white", nrow(dune))
cl[18] <- palette()[8]
ordilabel(mod, dis="si", cex=0.7, scaling="si", fill=cl)
points(mod, scaling="si", display="sp", cex=unlist(dune[18,]/2+0.4), pch=21, bg=4, col=1)
```
**Text:** SUs, **Dots:** species, *size:* abundance in SU 18

]

---
class: inverse center middle

# Interpretation: Ordination and External Variables

Gradient model is the deep idea of ordination: If sampling units are
close to each other in ordination, they occur in similar environments,
and if they are far away in ordiation, the environments differ. If our
ordination model is such that it can be assumed to be consistent with
gradient model (*i.e.*, it is able to handle unimodal species
responses), we can try to identify the underlying environmental
variables that cause the ordination structure.

---
# Fitted Vectors

.pull-left[

- Most popular method of displaying continuous variables
- Can show many variables in one graph
- Implies a **linear trend** surface, similarly as species arrows in PCA

- **Direction** shows the steepest change in variable: the direction
  of the **gradient**
  - Gradients are **not** parallel to axes: do **not** interpret axes, but gradients

- **Length** shows the relative importance or the strength of the variable

- The arrows is drawn from the origin and shows the direction of
  increase: there is equally strong **decrease to opposite** direction

]

.pull-right[
```{r envfit}
par(mar=c(4,4,1,1)+.1)
palette(viridis(8))
vmod <- cca(varespec)
plot(vmod, dis="si", type="n")
points(vmod, pch=21, col=1, bg=4)
ef <- envfit(vmod ~ ., varechem)
plot(ef, col=1, bg=8)
```
]
---
# Linear Trend Surface: Is it Adequate?

```{r ordisurf, fig.width=10}
palette(viridis(8))
par(mar=c(4,4,1,1)+.1, mfrow=c(1,2))
tmp <- ordisurf(vmod ~ Al, varechem, bubble=2.5, pch=21, col=4, bg=4, main="")
abline(h=0, v=0, lty=3)
plot(envfit(vmod ~ Al, varechem), col=1, bg=8)
tmp <- ordisurf(vmod ~ N, varechem, bubble=2.5, pch=21, col=4, bg=4, main="")
abline(h=0, v=0, lty=3)
plot(envfit(vmod ~ N, varechem), col=1, bg=8)
```

]

.pull-right[
]

---
# Significance tests

.pull-left[

- The strength of the fitted vector is given by the correlation $r$
  (or its square $r^2$) to the direction of the coordinates

- **Permutation tests:** Rows of environmental variables are shuffled
    into random order and $r^2$ is calculated

- The $P$-value is the probability that $r^2$ of random permutation is
  larger or equal to the observed value of fitted vector

- If this probability is low, say $P \le 0.05$, the variable is often
  regarded as significantly different from random

- Each variable is fitted independently, and other (correlated)
  variables do not influence its significance

]

.pull-right[
```{r echo=FALSE, results=TRUE}
ef$vectors
```
]

---
# Categorical Variables (Factors)

.pull-left[

- For one (and with tricks for two) factor we can show its value for
  each point

- The mean location or the **centroid** can be shown for several factors

- We also must assess the dispersion of points about the centroid
  &mdash; for this there are several graphical tools (next slide) for
  one factor in time

- Goodness of fit can be assessed as the proportion of total variance
  of points that can explained by the centroids

  - $r^2$ is the same statistic as for vectors, and for factors it
    gives the proportion of variance accounted for by the factor
    centroids



]

.pull-right[
```{r factor}
par(mar=c(4,4,1,1)+.1)
palette(viridis(8))
eff <- envfit(mod ~ Management, dune.env)
plot(mod, dis="si", type="n")
with(dune.env, points(mod, dis="si", pch=21, bg=viridis(4)[as.numeric(Management)]))
plot(eff, labels=levels(dune.env$Management), bg=8)
legend("topright", levels(dune.env$Management), pch=21, pt.bg=viridis(4), title = "Management")
```
]
---
# Displaying Factors

```{r factorshow, fig.width=8, fig.height=5.6}
par(mar=c(3,4,1,1)+.1, mfrow=c(2,2))
palette(viridis(8))
plot(mod, dis="si", type="n", main="Spider")
ordispider(mod, dune.env$Management, col=viridis(4), label=TRUE)
with(dune.env, points(mod, dis="si", pch=21, bg=viridis(4)[as.numeric(Management)]))
plot(mod, dis="si", type="n", main="Convex Hull")
ordihull(mod, dune.env$Management, draw="poly", col=viridis(4), label=TRUE)
with(dune.env, points(mod, dis="si", pch=21, bg=viridis(4)[as.numeric(Management)]))
plot(mod, dis="si", type="n", main="SD Ellipse")
ordiellipse(mod, dune.env$Management, col=viridis(4), draw="poly", label=TRUE)
with(dune.env, points(mod, dis="si", pch=21, bg=viridis(4)[as.numeric(Management)]))
plot(mod, dis="si", type="n", main="Enclosing Ellipse")
ordiellipse(mod, dune.env$Management, kind="ehull", col=viridis(4), label=TRUE)
with(dune.env, points(mod, dis="si", pch=21, bg=viridis(4)[as.numeric(Management)]))
```

---
# Significance Tests for Factors

.pull-left[
```{r echo=FALSE, results=TRUE}
eff$factors
```

- **Confidence ellipse** is derived from Standard Error ellipse and
    shows the likely location of the centroid
    - SD ellipse shows the dispersion of points

- If confidence ellipses do not overlap, they are approximately $\pm$
  different at the given level

]

.pull-right[
```{r confell}
palette(viridis(8))
par(mar=c(4,4,1,1)+.1)
plot(mod, dis="si", type="n", main="95% Confidence Ellipse")
ordiellipse(mod, dune.env$Management, kind="se", conf=0.95, draw="poly", col=viridis(4), label=TRUE)
with(dune.env, points(mod, dis="si", pch=21, bg=viridis(4)[as.numeric(Management)]))
```
]

---
# Lessons

- Fitted vectors are the most concise method of displaying the effects
  of continuous environmental variables, **but...**

- They imply a linear trend, and you should always check that this is adequate

- The environmental variables are not usually parallel to axes, but
  they are oblique

- You should interpret the directions of the gradients (and the arrow
  points to the steepest gradient) instead of axes

- You should **never** **interpret** **axes**, but direction in the
  ordination space

- Significance tests can help in interpretation of results, but they
  should not be trusted blindly (or not at all): they are only correlations

---
class: inverse center middle

# PCoA: Principal Coordinates Analysis

Principal Coordinates Analysis (PCoA) uses dissimilarities among SUs
instead of rectangular data of SUs and variables. If dissamilarities
are Euclidean distances, then PCoA is equal to PCA. However, the
method can be used with other dissimilarities, and often these are
useful for community analysis. PCoA is also known as metric or classic
multidimensional scaling.
---
# Ordination of Distances

- PCA was based on variances and linear projections of species when
  deriving principal components from regressions

- The same information can be extracted from distances among SUs

- More precisely, they are known as Euclidean distances in the species
  space

- Why is it called **Euclidean**?

- In species space, SUs are points on species axes that are at right
  angle to each others. The *difference* of species $j$ abundance for two
  SUs $i$ and $k$ is $y_{ij}-y_{kj}$ which is the *leg* or *cathetus* of a
  right-angled triangle, and the *squared* Euclidean distance of these two SUs
  over all *catheti* (species) is the squared *hypotenuse*
  $d_{ik}^2 = \sum_{j=1}^S (y_{ij}-y_{kj})^2$.

- It makes no sense to use Euclidean distances in PCoA, because PCA
  will give the same result more easily, but we can use other
  dissimilarities which are more useful in ecology

- These dissimilarities are handled similarly as Euclidean distances,
  but the results may be more useful (although there may be some
  geometrical glitches that we can usually ignore)

- All ordination methods we discuss are distance-based, and
  eigenvector methods (PCA, CA) are based on Euclidean distances or
  Euclidean distances of transformed data

---
# What is Wrong with Euclidean Distances?

- **They have no fixed upper limit,** but distances can vary among SUs
    that have nothing in common, and communities with high abundances
    appear more different than communities with low abundances

    - This can be cured with Euclidean distances of **transformed
      data**
    - For instance *Hellinger* or *Chord* distances have an
      upper limit if there are no shared species
      
    - If a distance can be expressed as a Euclidean distance of
      transformed data, it is better to use PCA with transformed data

- **They are based on squared differences** giving undue influence to
    exceptional high abundances

    - Instead of squared differences $(y_{ij}-y_{kj})^2$ we can use
      absolute differences $|y_{ij}-y_{kj}|$: *Manhattan* or
      *city-block* distances
      
    - Some dissimilarity measures combine both points: Bray-Curtis
      dissimilarity is based on absolute differences and is scaled to
      maximum value $1$:
      $d_{ik} = \frac{\sum_j |y_{ij}-y_{kj}|}{\sum_j (y_{ij}+y_{kj})}$

- **Research interest dictates** to use a specific non-Euclidean
    dissimilarity index
---
# Example: beta Diversity

.pull-left[

- We want to use Whittaker's beta diversity
  $\beta = \gamma/\bar \alpha -1$, where $\gamma$ is the number of species in
  pooled communities, and $\bar \alpha$ is the mean number of species in
  communities

- For two communities of $A$ and $B$ species with $J$ shared species, we have
  $\gamma = A+B-J$ and $\bar \alpha = (A+B)/2$ giving
$$\beta=\frac{A+B-J}{(A+B)/2}-1$$
$$~=\frac{2(A+B-J)}{A+B}-\frac{A+B}{A+B}$$
$$~=\frac{A+B-2J}{A+B}$$
   which is SÃ¸rensen dissimilarity

]

.pull-right[

```{r}
palette(viridis(8))
par(mar=c(4,4,1,1)+.1)
d <- designdist(varespec, "(A+B-2*J)/(A+B)", "binary")
dmod <- dbrda(d ~ 1)
plot(dmod)
```
]
---
# Example 2: Phylogenetic Distance

.pull-left[

- In normal dissimilarities, all species are regarded as equal

- If species are related, they would not contribute a full independent
  species to the dissimilarities

- Here we use the squareroot of Rao distance which is one of many
  phylogenetic dissimilarity indices (and they can equally well be
  used for related functional traits etc.)

- If species have diverged more than 65 Myr ago, they are regarded as
  completely independent

- PCoA is distance-based, and has no information on species, but these
  can be projected to the ordination afterwards similarly as species
  scores in PCA

]

.pull-right[
```{r}
palette(viridis(8))
par(mar=c(4,4,1,1)+.1)
plot(hclust(dune.phylodis), hang=-1, ylab="Time (Myr)", main="", sub="", xlab="")
abline(h=65, col=4)

```
]
---
# Example 2: Phylogenetic distance

```{r fig.width=11}
palette(viridis(8))
par(mar=c(4,4,1,1)+.1, mfrow=c(1,2))
m0 <- rda(dune)
pl <- plot(m0, type="n")
text(pl, "sites", pch=21, col=1, bg=1)
text(pl, "species", col=3, cex=0.7)
ordispider(m0, dune.taxon$Family, display="species", label=TRUE, col=3, cex=0.8)
d <- distrao(dune, dune.phylodis, dmax=65)
d <- sqrt(2*d) # raodist is 0.5*squared-Euclidean
md <- dbrda(d ~ 1)
sppscores(md) <- dune
pl <- plot(md, type="n")
text(pl, "sites", pch=21, col=1, bg=1)
text(pl, "species", col=3, cex=0.7)
ordispider(md, dune.taxon$Family, display="species", label=TRUE, cex=0.8)
```
---
class: inverse center middle

# NMDS: Nonmetric Multidimensional Scaling

PCA, CA & PCoA are eigenvector methods. They map community
dissimilarities linearly so that with the maximum number of dimensions
the dissimilarities are exactly represented, but some first dimensions
show as much as possible of the dissimilarities. NMDS is completely
different: it maps dissimilarities non-linearly onto low-dimensional
ordination. It assumes very little of data or response models and is
therefore more robust than eigenvector methods with their strict
parametric mapping.
---
# Eigenvector Methods: Linear Mapping

.pull-left[

- Eigenvector methods *decompose* observed dissimilarities
- With all possible axes, they reproduce exactly observed dissimilarities
- First axes show the largest possible amount of observed dissimilarities

- They map observed dissimilarities linearly so that they approach the
  original dissimilarities from below: each new axis *adds* to
  estimated dissimilarities

- Eigenvector methods may have their fixed way of defining observed
  dissimilarities: **PCA** uses Euclidean distances and **CA** uses
  $\chi$-distances (which are Euclidean distances of
  $\chi^2$-transformed data)

]

.pull-right[

```{r}
par(mar=c(4,4,1,1)+.1)
palette(viridis(8))
stressplot(cca(mite), l.col=2, p.col=6)
```
Oribatid mites: 2-dim CA of $\chi$-distances

]

---
# NMDS: Monotone Mapping

.pull-left[

- NMDS maps dissimilarities non-linearly onto low-dimensional ordination

- **Monotone regression**: Euclidean distances of points in the
    ordination space are **rank-order similar** to community
    dissimilarities
    - The **ordination space** is metric, but the **regression** is non-metric

- Rank-order similar: if *observed dissimilarity* $d_a < d_b$ then
  Euclidean *ordination distance* $\delta_a < \delta_b$
  
- The rank orders of observed dissimilarities cannot be exactly
  preserved by rank-orders of ordination distances in low-dimensional
  solutions, and this causes **stress**

- **Stress:** scatter of observed dissimilarities against
    expected monotone regression

]

.pull-right[

```{r}
par(mar=c(4,4,1,1)+.1)
palette(viridis(8))
mmod <- metaMDS(mite, trace=FALSE, parallel=2)
stressplot(mmod, l.col=2, p.col=6)
```
.small[Oribatid mites: NMDS in 2D]

]
---
# Advantages of NMDS

- Eigenvector methods can give good ordination if the inherent
  ordination model is similar to the model species response to
  gradients

  - PCA model is linear and poor, but CA can cope with some regular
    unimodal response models and give good results

- NMDS does not assume any specific response model, but it can adjust
  to many models &mdash; even non-regular &mdash; and this makes it
  **robust**

- Robust: assumes little of data and tolerates violations of
  assumptions

- Can use ecological more meaningful dissimilarities than Euclidean
  distances and $\chi$-distances of PCA and CA
  
  - Shares this feature with PCoA (which still applies Euclidean
    mapping to these dissimilarities)

- Champion in tests with simulated data with non-regular unimodal
  species responses

---
## How Eigenvector Methods Differ from NMDS?

- **Origin:** shows the average (expected) community composition in
    eigenvector methods, but has **no** special meaning in NMDS

    - It is convenient to have the origin in the middle of the
      ordination in NMDS, but it means nothing: All that means is the
      configuration of points and distances between points &mdash; and
      these do not depend on the location of the origin

- **First axis** shows the major variation of the *data* (in terms of
    inertia, either variance of $\chi^2$) in eigenvector methods, but
    has **no** special meaning in NMDS

    - It is convenient to rotate NMDS space so that the first axis
      shows the major variation in the *ordination*, but it means
      nothing: All that means is the configuration of points and
      distances between points &mdash; and these do not depend on the
      directions of the axes

    - For convenience NMDS axes are often rotated to Principal
      Components, but they can be rotated to be parallel to
      environmental variables or any other direction

- What is in common is that NMDS and Eigenvector methods (PCA, CA, PCoA)
  have metric ordination space that can be interpreted in the same way
  for the configuration of points and using environmental variables
---
# These NMDS Ordinations are Identical

```{r fig.width=11}
palette(viridis(8))
par(mar=c(4,4,1,1)+.1, mfrow=c(1,2))
plot(mmod, type="n")
text(mmod, dis="si", col=1, cex=0.7)
text(mmod, dis="sp", col=5, cex=0.6, xpd=TRUE)
plot(envfit(mmod ~ SubsDens + WatrCont + Shrub, mite.env), col=3, bg=8, xpd=TRUE)
mrot <- MDSrotate(mmod, mite.env$WatrCont)
plot(mrot, type="n")
text(mrot, dis="si", col=1, cex=0.7)
text(mrot, dis="sp", col=5, cex=0.6, xpd=TRUE)
plot(envfit(mrot ~ SubsDens + WatrCont + Shrub, mite.env), col=3, bg=8, xpd=TRUE)

```
---
## Abernethy: Dissimilarity & Mapping

```{r abergrid, fig.width=8,fig.height=5.5, results=FALSE}
palette(viridis(8))
par(mar=c(4,4,1,1)+.1, mfrow=c(2,2))
#x <- rbinom(prod(dim(mtfit)), 1, mtfit)
#dim(x) <- dim(mtfit)
x <- abernethy[,1:36]
m1 <- rda(decostand(x, "hell"), scale=FALSE)
pl <- plot(m1, dis="si", scaling="si", type="n", main="PCA: Hellinger")
tmp <- ordisurf(pl, abernethy$Age, add=T, col=4)
grp <- rep(1, nrow(x)) # for ordiarrows
ordiarrows(pl, grp, col=7)
points(pl, dis="si", pch=16, col=2)
m2 <- cca(x)
pl <- plot(m2, dis="si", scaling="si", type="n", main="CA: Chi")
tmp <- ordisurf(pl, abernethy$Age, add=T, col=4)
ordiarrows(pl, grp, col=7)
points(pl, dis="si", pch=16, col=2)
m3 <- dbrda(vegdist(wisconsin(x))~1)
pl <- plot(m3, dis="si", scaling="si", type="n", main="PCoA: Bray-Curtis / Wisconsin")
tmp <- ordisurf(pl, abernethy$Age, add=T, col=4)
ordiarrows(pl, grp, col=7)
points(pl, dis="si", pch=16, col=2)
m4 <- metaMDS(vegdist(wisconsin(x)), trace=FALSE)
m4 <- MDSrotate(m4, abernethy$Age)
pl <- plot(m4, dis="si", scaling="si", type="n", main="NMDS: Bray-Curtis / Wisconsin")
tmp <- ordisurf(pl, abernethy$Age, add=T, col=4)
ordiarrows(pl, grp, col=7)
points(pl, "sites", pch=16, col=2)
```
---
## Stress Plot Indicates NMDS Does Best

```{r aberstress, fig.width=8,fig.height=5.5, results=FALSE}
par(mar=c(4,4,1,1)+.1, mfrow=c(2,2))
palette(viridis(8))
stressplot(m1, l.col=2, p.col=6, main="PCA: Hellinger")
stressplot(m2, l.col=2, p.col=6, main="CA: Chi")
stressplot(m3, l.col=2, p.col=6, main="PCoA: Bray-Curtis / Wisconsin")
stressplot(m4, l.col=2, p.col=6, main="NMDS: Bray-Curtis / Wisconsin")
```
---
# Is NMDS Credible?

.pull-left[

- NMDS is the only method that does not produce a curve &mdash; but is
  this credible?

- There is a cycle in SU sequence in the oldest times. Species scores
  indicate that the cycle is *Rumex* &mdash; *Anthyllis vulneraria*
  &mdash; Graminoids &mdash; *Artemisia* and then going to the linera
  succession *Juniperus communis* &mdash; *Betula* &mdash; *Pinus
  sylvestris* &mdash; *Alnus glutinosa*

- This looks like a credible cycle of pioneer communities (*Rumex*)
  and tundra steppes (*Artemisia*) before going to regular forest
  succession

- All other methods had a curve that hid the pioneer cycle

]

.pull-right[

```{r abernmds}
par(mar=c(4,4,1,1)+.1)
palette(viridis(8))
sppscores(m4) <- x
pl <- plot(m4, type="n")
text(pl, "sites", labels=abernethy$Age, cex=0.6)
ordiarrows(pl, rep("a",nrow(abernethy)), col=7)
ordilabel(pl, dis="sp", cex=0.7, labels=make.cepnames(colnames(x)), priority=colSums(x), col=3)
```

]