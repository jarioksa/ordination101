---
title: "Basic Ordination Methods"
subtitle: "Ordination for Ecological Methods"
author: "Jari Oksanen"
date: "Nov 8 to 15, 2017"
output:
  xaringan::moon_reader:
    css: ['default', './resources/gavins.css']
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(comment=NA,
                      echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE,
                      fig.align = 'center', fig.height = 5, fig.width = 5.5, dev = 'svg')
knitr::knit_hooks$set(crop.plot = knitr::hook_pdfcrop)
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(rgl.newwindow = TRUE)
```
```{r packages, include = FALSE, cache = FALSE}
require('vegan')
if (packageVersion('vegan') < '2.5.0') # in github
   stop("needs vegan 2.5-0")
data('varespec', 'varechem', 'mite', 'mite.env', 'dune', 'dune.env')
data('dune.phylodis','dune.taxon')
require('natto') # in github, natto::raodist()
require('vegan3d')
if (packageVersion('vegan3d') < '1.1.1') # in github
   stop("needs vegan3d 1.1-1")
require('labdsv')
data('bryceveg')
require(analogue)
data(abernethy)
library('mgcv')
require('knitr')
require('viridis')
```
Source files of these slides are [in github](https://github.com/jarioksa/ordination101).

<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.

© Jari Oksanen 2017

This version was built on `r date()`

<!-- ********** INTRO ************ -->
---
# This Is Ordination

.center[
```{r whatisthis, fig.height=5.3}
par(mar=c(4,4,0,0)+.1)
palette(viridis(8))
foo <- cca(varespec ~ Al + P + K, varechem)
plot(foo, type="n")
points(foo, dis="si", pch=21, col=2, bg=6)
ordilabel(foo, dis="sp", col=4, cex=.8, priority=colSums(varespec))
text(foo, dis="cn", col=2, cex=1.6, lwd=2)
```
]
---
class: inverse center middle

# Ordination: Why and What?
---
# Multivariate Data

- Ordination methods are multivariate graphical tools to display
  multivariate data

- Multivariate data have several variables on several sampling
  units (SU), and such data are best analysed with multivariate
  methods

- We do not have one obvious dependent variable in multivariate data,
  but we would still want to understand the major features of the data

  - Species abundances in sampling sites, OTUs of DNA sequence
    analysis in sampling units, measurements of chemical and physical
    conditions in sampling stations *etc*

- If we have a clear hypothesis with a clear dependent variable, it is
  better to use univariate methods

- Sometimes we can summarize multiple variables into one indicator
  (*e.g*, diversity), and then we can use univariate methods

- If all else fails, we *must* use multivariate methods

---
# Multivariate Data and Display

.pull-left[
.small[
```{r echo=FALSE}
varespec
```
]
]

.pull-right[
```{r mvexample, results=FALSE}
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
m <- metaMDS(varespec)
m <- MDSrotate(m, varechem$Humdepth)
ef <- envfit(m, varechem)
plot(m, type="n")
text(m, dis="si", col=1, cex=0.8)
text(m, dis="sp", col=5, cex=0.8)
plot(ef, col=3, p.=0.05)
```
]
---
# Reading Ordination Plots

.pull-left[

- The ordination graphs shows the main features of similarities and differences in the data

- If two SUs are close to each other, they have similar communities
- If two SUs are far away from each other, the communities differ

- If two species are close to each other, they have similar occurrence
  patterns, and occur most abundantly in SUs that are close to them

- It can be assumed that environmental conditions are behind these
  differences and we can identify the external variables that explain
  the ordination structure

]

.pull-right[
```{r mvexample2}
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
# redraw the previous
plot(m, type="n")
text(m, dis="si", col=1, cex=0.8)
text(m, dis="sp", col=5, cex=0.8)
plot(ef, col=3, p.=0.05)
```
]

---
# Why Ordination?

- Ordination provides a graphical summary of main structure in the data

- Instead of looking at a huge number of variables with a huge number
  of observations, we compress multidimensional data into two (or
  three) dimensions that we can easily inspect and comprehend

- Ordination is **descriptive**, but it can help us to understand the
  main features of the data

- Often ordination is *private:* we familiarize with the data so that
  we can formulate research hypotheses, check the adequacy of our
  hypothesis, and communicate the results to other people

  - *Private:* we do it, but we do not tell anybody what we do in our privacy

  - Ordination need not be published or shown to anyone to be useful to you

- Sometimes we can build rigorous tests on ordination results: these
  summarize the major structure and allow using simpler test
  procedures

  - Some ordination methods allow testing within themselves

- Many people never need ordination, but some do &mdash; that all
  depends on your research questions

---
# The Purpose of These Lectures

- Provide information on ordination for those who may need it

- Even if you do not need them yourself, it is useful to understand
  how to interpret publications that use ordination

- These lectures are not about using ordination software, but they are
  about the **methods**

  - There are several alternative software implementations to perform
    these analyses, although the lecture slides were written using
    **R** statistical language and mainly with **vegan** package

  - There are no **R** commands in these slides, but if you *want* to
    see them, look at the
    [source code](https://github.com/jarioksa/ordination101)
    of these lectures

- I avoid mathematics, but also over-simplification and
  superficiality: I try to *explain* how these methods work in terms
  of common statistical tools, for instance, regression analysis (and
  that is about how complicated it becomes)

- I would be really pleased if you understand what I say: Please
  interrupt me and ask as soon as you get lost

---
# The Structure of These Lectures

In the first part of lectures I describe four main basic ordination
methods:

- Principal Components Analysis (PCA) is just an extension of
  regression analysis, and a simple summary of the data. I also
  explain why it fails often with ecological community data.

- Correspondence Analysis (CA) is a variant of PCA, but that small
  difference makes it much better for community data.

- In Principal Coordinates Analysis (PCoA) the main focus is what we
  actually mean with things being similar, and how we can use
  ecological judgement in defining similarity.

- Nonmetric Multidimensional Scaling (NMDS) is presented as a flexible
  and robust tool that often is most reliable in finding interpretable
  results.

- I also explain how to interpet ordination results with the help of
  external information, such as measurements of chemical or physical
  parameters and other environmental conditions

---
# The Structure of These Lectures

In the latter part of lectures I describe how to use external
environmental variables in constraining ordination. These are
particularly useful if we have controlled experiment and we want to
analyse the statistical significance of multivariate response. Even
with survey data, the use of external variables can make
interpretation much easier. Some of the main themes are:

- How constrained ordination is defined as an extension of regression
  analysis, and how it can be applied in the context of PCA, CA and
  PCoA.

- How we can test the statistical significance of external variables
  in multivariate setting.

- How we can condition the analysis for some background variables and
  remove their effects before main ordination.

- How we can partition multivariate response to different sources
  variation.

<!-- ********** PCA ************** -->
---
class: inverse center middle

# PCA: Principal Components Analysis

PCA  is the  basic ordination  method and  the one  that Statisticians
know. It is poor for most uses,  but we need to know PCA to understand
how other methods improve upon it.  The main problem of PCA is that it
is  linear, but  community  data are  nonlinear.
---
# From Regression to Ordination

- What is the best possible predictor variable for a species? --- It
  is the species itself!

- Some species can also be powerful in predicting abundances of
  other species

- The best possible predictor is such a (linear) combination of
  species that explains abundances of all species as well as possible

- That linear combination is the first PCA axis
---
# 3 Cryptogams in Reindeer Pastures

```{r bottpredict, fig.width=8, fig.height=5.5}
bott <- varespec[, c("Pleuschr","Cladrang","Cladstel")]
bott0 <- scale(bott, scale = FALSE)
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
panel.lm <- function(x, y, col.smooth = 1, tcex=1.5, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    points(x, y, ...)
    mod <- lm(y ~ x - 1)
    txt <- paste0("Resid.Var = ", round(summary(mod)$sigma^2, 1))
    abline(mod, col = col.smooth, lwd=2)
    text(mean(usr[1:2]), 0.9*usr[4], txt, cex=tcex)
}
pairs(bott0, pch=16, col = 4, ylim = range(bott0), panel = panel.lm)
```
---
# Best Predictor
```{r bottpc1, fig.width=10, fig.height=4}
mod <- rda(bott0)
koef <- mod$CA$v[,1]
xlab = paste(formatC(koef,digits=2, flag="+"), names(koef), sep="*",collapse="")
PC1 <- (bott0 %*% mod$CA$v)[,1]
df <- data.frame(bott0)
palette(viridis(8))
par(mar=c(4,4,0,1)+.1, mfrow=c(1,3))
ylim <- range(bott0)
plot(Pleuschr ~ PC1, df, type="n", ylim=ylim, xlab=xlab, cex.lab=1.2)
panel.lm(PC1, df$Pleuschr, pch=16, col=2, col.smooth=4)
plot(Cladrang ~ PC1, df, type="n", ylim=ylim, xlab=xlab, cex.lab=1.2)
panel.lm(PC1, df$Cladrang, pch=16, col=2, col.smooth=4)
plot(Cladstel ~ PC1, df, type="n", ylim=ylim, xlab=xlab, cex.lab=1.2)
panel.lm(PC1, df$Cladstel, pch=16, col=2, col.smooth=4)

```
---
# What Was Explained?

- PCA explains the **variance** of the data: the squared
  differences between observed values $y$ and species means $\bar y$:
  $\mathrm{Var}(y_j) = \sum_{i=1}^N(y_{ij} - \bar y_j)^2/(N-1)$
  - NB., ordination always centres data so that $\bar y = 0$

- In the example, the variances of three species were
  `r round(apply(bott0, 2, var), 1)`, with sum
  `r round(mod$tot.chi, 1)`, and the sum of residual variances of the PC
  regression was `r round(mod$tot.chi - mod$CA$eig[1], 1)`

- The variance explained by the first PC is the **eigenvalue** of the
  axis, and it is the difference of total variance and residual
  variance $\lambda_1 =$ `r round(mod$CA$eig[1], 1)` which is
  `r round(100*mod$CA$eig[1]/mod$tot.chi, 1)`% of total variance

- The coefficients (`r round(mod$CA$v[,1], 3)`) of three species are
  the **species scores** used in ordination diagrams, and they are also
  regression coefficients that project centred species abundances to
  the PC
  - NB., ordination always centres data so that the intercept
  is zero

- The result of this projection are the sampling unit **(SU) scores**
  that are used in ordination diagrams -- this is also called as the
  **principal component**

---
# Second and Further Principal Components

- Second PC has similar components, but is **orthogonal** to the
  previous one:

- Species scores and SU scores are orthogonal to (uncorrelated with)
  corresponding scores in the previous axes

- The second PC explains the largest possible amount of remaining
  variance
  - In the example, its eigenvalue is $\lambda_2 =$ `r round(mod$CA$eig[2], 1)`
    which is  `r round(100*mod$CA$eig[2]/mod$tot.chi, 1)`% of total variance

- The sum of all $K$ eigenvalues is equal to the variances of all
   species $\sum_{k=1}^K \lambda_k = \sum_{j=1}^S \mathrm{Var}(y_j)$:
   The axes decompose variance into independent ordered components

- In modern software, we do not find second and further PCs one-by-one
  with orthogonalization, but the interpretation of the results is
  still similar

---
# Is This Circular?

- We explain species by species &mdash; and this sounds badly circular

- We perform no statistical testing, and what looks regression
  actually is **rotation** of species data

- We only like to find a way of looking at the data so that some first
  (say PC1 and PC2) show as much of the variance of the data as possible

- Then we can ignore the later axes and say that a **major part** of
  variation is displayed in the first dimensions that we
  can graph

- PCA is **not** a statistical method, but it is only a rotation of
  the data

---
# Species Scores in Multidimensions

.pull-left[

- We had species scores for a single axis, but in 2 and more dimensions
  we must look at all dimensions simultaneously

- The coefficients together define the **direction** to which the
  species abundance **increases** most steeply &mdash; and it
  **decreases** to the opposite direction

- The response is linear in 2D: the contours of linear trend surface are
  perpendicular to the arrow

- The estimated abundances of all species on each SU can be read by
  projecting the SU point to the arrow: PCA **approximates** data

- Species were centred: The zero contour goes through the origin

]

.pull-right[
```{r plot3sp}
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
biplot(mod, type = c("text","points"))
tmp <- ordisurf(mod ~ Cladstel, df, add = TRUE, col=4, knots=1)
```
]

---
# Distance and Direction From the Origin

.pull-left[

- In the origin, each species occurs at its average abundance $\bar y_j$

- The further away from the origin SU is situated, the more strongly
  species abundances differ from the average

- For species the angles between arrows indicate the similarity of
  change, and the length of the arrow the steepness of the change:
  right angle means no relation, zero angle identical response, and
  direct angle opposite responses

- Try yourself: where these species are most abundant and scarcest?
  Which species attain highest abundances and where? Which species
  replace each other and which are independent from each other?
]

.pull-right[

```{r plot3sp2}
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
biplot(mod, col=c(4,1))
```

]
---
# Rotation in Species Space

.pull-left[

- In species space every species is an axis

- Each axis is at right angle against all other axes

- The SUs are points, and the abundance of each species gives its location
  in species axes

- There can be a huge number of species axes, and we cannot see them all

- We rotate the species space so that we see as much as possible from the 
  locations of the points: This is the **PCA!**

]

.pull-right[
```{r specspace}
palette(viridis(8))
ordiplot3d(bott0, mar=c(4,4,0,2), angle=20, pch=16, col=2, ax.col=4)
```
[Please Click Me!](http://cc.oulu.fi/~jarioksa/opetus/metodi/3D/PCArotation/index.html)

]
---
# Complete Reindeer Pasture Data

.pull-left[

- Analysis of complete data not too different from just three species

- PCA analyses variances and shows absolute differences from the mean
  $y - \bar y$: minor species change little in absolute units

- Actually only some few species influence results, and the data are
  not very multivariate

]

.pull-right[
```{r pcavare}
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
modf <- rda(varespec)
biplot(modf, col=c(4,1))
```
]

---
# Explaining Variance

.pull-left[

- Abundant species have high variance, and PCA explains variance

- It may be very easy to explain variance, when only a few species
  account for the most, because then data are not very
  multidimensional

- Minor species may be ecological indicators, but they have little
  effect on community ordination with PCA

- Three abundant cryptogams accounted for
  `r round(100*mod$tot.chi/modf$tot.chi, 1)`% of total variance in the complete
  data set, and are dominant in PCA

]

.pull-right[

```{r varabu}
par(mar=c(4,4,0,1)+.1)
v <- apply(varespec, 2, var)
plot(colMeans(varespec), v, xlab="Mean Abundance", ylab="Variance", type="n")
ordilabel(cbind(colMeans(varespec), v), priority = v, fill = "yellow", cex=1)
```

]
---
# Equal Weights to All Species

.pull-left[

- Use correlation instead of variance and minimize residuals $1-R^2$ in
  PC regression: This gives equal weights to all species

- Equalized data are more multidimensional, and scarce species will
  also influence the result, and the ordination changes from the previous

- PCA implies that species effects should be shown by arrows, but this
  makes graphs very messy, and often it makes sense to drop arrows and
  just show the species scores at arrowheads

- They still should be interpreted like arrows: **Distance** and
  **direction** from the origin counts

]

.pull-right[
```{r pcacorr}
mod1 <- rda(varespec, scale=TRUE)
palette(viridis(8))
par(mar=c(4,4,0,1)+.1)
plot(mod1, scaling="sites", type="n")
text(mod1, scaling="sites", dis="site", col = 1, cex=0.8)
text(mod1, scaling="sites", dis="species", col=4, cex=0.8)
```
]
---
class:  inverse center middle

## PCA Trouble: It is Linear!
---
# Linear Method

- PCA builds upon the idea of linear regression

- PC axes are sometimes called as latent variables: unknown and hidden
  real variables that we dig up from data

- Community response to environmental variables is non-linear: PC axes
  cannot find such variables

```{r mtfgrad, fig.width=10, fig.height=3}
source('data/mtfit.R')
palette(viridis(6))
par(mar=c(4,4,0.4,1)+.1)
matplot(altfit, mtfit, type="l", lty=1, xlab="Altitude (m)", ylab="Species Response")
```
Mt Field (Tasmania): Fitted Responses
---
# Horseshoe: Beware!

.pull-left[

- If species have nonlinear, unimodal responses to gradients, the
  gradient appears as a curve: **horseshoe artefact**

- People often want to interpret axes as latent variables, but even a
  single gradient is not an axis: **Beware**

- Even if you are aware of the risk of horseshoe, it can hide other
  shorter gradients: Using two dimensions for one gradient is wasteful
  and confusing

- In general, PCA **should not be used** for community data

- PCA is useful for linear data, or for deriving new combined
  variables for other analyses when PCA explains a large proportion of
  variance

]

.pull-right[
```{r horseshoe}
mod <- rda(mtfit)
palette(viridis(8))
par(mar=c(4,4,0,1)+.1)
plot(mod, dis="si", scaling="si", type="n")
points(mod, dis="si", scaling="si", pch=16, col=2)
```
PCA of the expected species responses of previous slide
]
---
# Horseshoe is Common

```{r morehorses, fig.width=11}
palette(viridis(8))
par(mfrow=c(1,3), mar=c(4,4,2,1)+.1)
m1 <- rda(varespec)
m2 <- rda(bryceveg)
m3 <- rda(mite)
plot(m1, dis="si", type="n", main="Reindeer Pastures")
points(m1, dis="si", pch=16)
plot(m2, dis="si", type="n", main="Bryce Canyon")
points(m2, dis="si", pch=16)
plot(m3, dis="si", type="n", main="Oribatid Mites")
points(m3, dis="si", pch=16)
```
---
# When Would We Expect a Horseshoe?


- **Always** with community data

- Even when we cannot see the horseshoe, it may be hidden below the
  crumble of noisy analysis

- Horseshoe can hide underlying gradients and disperse their effect
  over several PCs so that they cannot be found and interpreted

- Horseshoe can be expected also in homogeneous data with short
  gradients

---
# Niche for PCA

.pull-left[

- New synthetic variables from correlated measurements to avoid
  problems of collinearity in statistical analysis

  - Using PCs as explanatory variables in regression

- For this PCA should explain a large amount of variance

- Use only the subset of correlated variables that may be assumed to
  describe the same underlying (latent) variable: **measurement
  model**

- Use correlations when variables are not measured in the same units

- Do not use axes directly: they may not be parallel to interpretable
  variables

]

.pull-right[

```{r latent}
mod <- rda(varechem, scale=TRUE)
palette(viridis(8))
par(mar=c(4,4,1,1)+.1)
biplot(mod, display="sp", col=1)
```
.small[
Soil data for reindeer pastures
]
]

<!-- ********* CA ************ -->
---
class: inverse center middle

# CA: Correspondence Analysis

Correspondence Analysis is an eigenvector method just like PCA, and it
can be seen as its variant. However, this small difference makes it
handle nonlinear species responses better than PCA. For community
data, CA is usually a much better alternative than PCA.

---
# Correspondence Analysis

- In PCA we explain the differences of species from their averages
  $y_{ij} - \bar y_j$ using thier mean squares (variance) as a measure
  of goodness of fit

- In CA the expected value is derived both from SUs and species,
  and the goodness of fit is $\chi^2$

- We first scale the observed data so that their sum is one
  $\left( \sum_{i=1}^N \sum_{j=1}^S y_{ij} = 1 \right)$
  &mdash; that is, we divide data with its total

- If all communities have identical species composition, and all
  species occur in equal proportion in all SUs, then the expected
  abundance is $r_i c_j$ where $r_i$ is row (SU) sum and $c_j$ is the
  column (species) sum: This takes the place of $\bar y_j$ of PCA

- We look at proportional difference
  $(y_{ij} - r_i c_j)/\sqrt{r_i c_j}$, and its square is
  $\chi^2 = \sum_i \sum_j (y_{ij} - r_i c_j)^2/(r_i c_j)$:
  This takes the place of variance of PCA

- Then we proceed with regression to find the axes, just like in PCA
  (except that we need to use weights $\sqrt{r_i}$, $\sqrt{c_j}$)

---
# Small But Important Difference

.pull-left[

- CA is based on similar regression as PCA, but with
  $\chi$-standardized data and weights

- Regression is linear, but *observed* values of data rather pack
  together so that high values are close to each other along the axis

- The response of species can be approximately unimodal: This matches
  the gradient model of species responses

- The solution can be better related to environmental variables than in PCA

- CA is still **not** a completely unimodal ordination and it still
  shows a curve artefact, but not as garbled as PCA

]

.pull-right[

```{r cagam}
palette(viridis(3))
par(mar=c(4,4,1,1)+.1)
mod <- cca(bott) # from PCA lecture
CA1 <- mod$CA$u[,1]
matplot(CA1, bott, pch=16, xlab = "CA axis 1", ylab="Species Abundance")
legend("top", colnames(bott), pch=16, lty=1, col=1:3, bty="n")
i <- order(CA1)
for(k in 1:3) {
   m <- gam(bott[,k] ~ s(CA1, k=4), family=quasipoisson)
   lines(CA1[i], fitted(m)[i], lwd=2, col = k)
}
```
]

---
# PCA Orders Linearly, CA Packs

.pull-left[
```{r pcatabasco}
tabasco(varespec, rda(varespec), scale="log", col=viridis(10))
```
**PCA:** Rows and columns ordered by PC1
]

.pull-right[
```{r catabasco, }
tabasco(varespec, cca(varespec), scale="log", col=viridis(10))
```
**CA:** Rows and columns ordered by CA1
]

---
# More Clearly in Sparse Data

.pull-left[
```{r pcatabadune}
tabasco(dune, rda(dune), col=viridis(5))
```
**PCA:** Dutch Dune Meadows
]

.pull-right[
```{r catabadune, }
tabasco(dune, cca(dune), col=viridis(5))
```
**CA:** Diagonally structured table
]
---
# CA Preserves Order

- CA can also have a curve artefact with dominant long gradient, but
  it is not garbled and involuted like the PCA horseshoe

- First CA axis preserves the correct ordering: CA is a **seriation method** 

```{r abernethy, fig.width=8, fig.height=3.5}
par(mar=c(4,4,0.2,1)+.1, mfrow=c(1,2))
palette(viridis(8))
m0 <- rda(abernethy[,1:36])
m1 <- cca(abernethy[,1:36])
plot(m0, dis="si", scaling="si", type="n")
ordiarrows(m0, rep("a", nrow(abernethy)), scaling="si", col=7)
points(m0, dis="si", scaling="si", pch=16, col=1)
plot(m1, dis="si", scaling="si", type="n")
ordiarrows(m1, rep("a", nrow(abernethy)), scaling="si", col=7)
points(m1, dis="si", scaling="si", pch=16, col=1)
```
.pull-right[
Abernethy Forest pollen data (5515 to 12145 BP)
]
---
# Unimodal Responses in 2D

.pull-left[
- CA packs species also in multidimensional space

- Instead of an arrow of increase, the species score may be regarded
  as a centre of abundance

- It is said that CA approximates the unimodal model

- We may think that the species scores gives the species maximum and
  the abundance decreases to every direction from the centroid given
  by the species score

- In PCA species close to the origin changed little and was poorly
  presented by the ordination, but in CA it really may have its
  optimum there

]

.pull-right[
```{r cascores, fig.height=5.5}
par(mar=c(4,4,0.9,0.5)+.1, mfrow=c(2,2))
palette(viridis(8))
mod <- cca(dune)
with(dune, tmp <- ordisurf(mod ~ Poaprat, bubble=2, family=quasipoisson, knots=2, col=6, scaling="si", main="Poa pratensis"))
with(dune, tmp <- ordisurf(mod ~ Trifrepe, bubble=2, family=quasipoisson, knots=2, col=6, scaling="si", main="Trifolium repens"))
with(dune, tmp <- ordisurf(mod ~ Lolipere, bubble=2, family=quasipoisson, knots=2, col=6, scaling="si",main="Lolium perenne"))
with(dune, tmp <- ordisurf(mod ~ Juncarti, bubble=2, family=quasipoisson, knots=2, col=6, scaling="si", main="Juncus articulatus"))
```
]

---
# Numbers and Interpretation

.pull-left[

- Sum of all eigenvalues $\sum_k \lambda_k = \chi^2$, and
  $\lambda_k / \chi^2$ gives the proportion of inertia explained by the axis

- This inertia is **not** variance but $\chi^2$, and eigenvalues or percentages
  cannot be compared with PCA

- The eigenvalue is bound to be $\lambda \le 1$

- High eigenvalue is good, but too high (say, $\lambda > 0.7$) are
  suspicious: disjunct or partly disjunct data where a group of SUs
  shares hardly anything (or nothing) with others

- Species scores can be seen as points indicating species optima
  instead of arrows of increase in PCA

- Both species and SU scores are **weighted averages** of each other
  &mdash; but exact relationship depends on scaling in graphs

]

.pull-right[

```{r caplot}
palette(viridis(8))
par(mar=c(4,4,0,1)+.1)
plot(mod, scaling="sites", type="n")
text(mod, scaling="sites", dis="site", col = 1, cex=0.8)
text(mod, scaling="sites", dis="species", col=4, cex=0.8)
```

]

---
# Weighted Averages and Scaling

.pull-left[
```{r caspecwa}
par(mar=c(4,4,1,1)+.1)
palette(viridis(8))
with(dune, tmp <- ordisurf(mod ~ Lolipere, bubble=2.5, family=quasipoisson, knots=2, main="scaling='species'", pch=21, col=1, bg=4))
text(mod, dis="sp", cex=0.8, col=3)
with(dune, ordispider(mod, Lolipere>0, w=Lolipere, label=FALSE, show=TRUE, col=4))
 with(dune, plot(envfit(mod ~ rep("A",nrow(dune)), w=Lolipere), labels="Lolipere", bg=8))
```
**Dots:** SUs, size: *Lolium perenne*, **Text:** species
]

.pull-right[
```{r casitewa}
par(mar=c(4,4,1,1)+.1)
palette(viridis(8))
plot(mod, scaling="si", type="n", main="scaling='sites'")
ordispider(mod, unlist(dune[18,]>0), scaling="si", w = unlist(dune[18,]), show=T, display="sp", col=4)
cl <- rep("white", nrow(dune))
cl[18] <- palette()[8]
ordilabel(mod, dis="si", cex=0.8, scaling="si", fill=cl)
points(mod, scaling="si", display="sp", cex=unlist(dune[18,]/2+0.4), pch=21, bg=4, col=1)
```
**Text:** SUs, **Dots:** species, size: abundance in SU 18

]
---
# Rare Species

.pull-left[

- Rare species are often extreme in CA

- CA explains deviation from expected abundance, and it expects every
  species occur at every SU &mdash; rare species occur only in one or
  two

- CA assesses deviation as proportional to expected &mdash; rare
  species have low expected values and look extreme

- CA is weighted, and rare species have low weights &mdash; they do
  not influence ordination of SUs or other species very much

- Some people tell you to routinely remove rare species from the data,
  but I do not recommed doing so

]

.pull-right[
```{r carare}
par(mar=c(4,4,0,1)+.1)
cl <- rev(viridis(5))
fr <- colSums(bryceveg > 0)
fr <-  cut(fr, breaks=c(0,1,3,7,15,100))
bmod <- cca(bryceveg)
plot(bmod, dis="sp", type="n")
points(bmod, dis="sp", pch=21, bg=cl[fr])
legend("topleft", c(" 1"," 2 - 3"," 4 - 7"," 8 - 15","16 - 87"), pch=21, pt.bg=cl, title="Species Frequency")
```
Bryce Canyon, species scores
]
---
class: inverse center middle

# Interpretation: Ordination and External Variables

Gradient model is the deep idea of ordination: If sampling units are
close to each other in ordination, they occur in similar environments,
and if they are far away in ordination, the environments differ. If our
ordination model is such that it can be assumed to be consistent with
gradient model (*i.e.*, it is able to handle unimodal species
responses), we can try to identify the underlying environmental
variables that cause the ordination structure.

---
## Fitted Vectors

.pull-left[

- Most popular method of displaying continuous variables

- Can show many variables in one graph

- Implies a **linear trend** surface, similarly as species arrows in PCA

- **Direction** shows the steepest change in variable: the direction
  of the **gradient**
  - Gradients are **not** parallel to axes: do **not** interpret axes, but gradients

- **Length** shows the relative importance or the strength of the variable

- The arrow is drawn from the origin and shows the direction of
  increase: there is equally strong **decrease to opposite** direction

]

.pull-right[
```{r envfit}
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
vmod <- cca(varespec)
plot(vmod, dis="si", type="n")
points(vmod, pch=21, col=1, bg=4)
ef <- envfit(vmod ~ ., varechem)
plot(ef, col=1, bg=8, cex=1)
```
Reindeer Pastures and Soil data
]
---
# Linear Trend Surface: Is it Adequate?

```{r ordisurf, fig.width=10}
palette(viridis(8))
par(mar=c(4,4,0,1)+.1, mfrow=c(1,2))
tmp <- ordisurf(vmod ~ Al, varechem, bubble=2.5, pch=21, col=4, bg=4, main="")
abline(h=0, v=0, lty=3)
plot(envfit(vmod ~ Al, varechem), col=1, cex=1, bg=8)
tmp <- ordisurf(vmod ~ N, varechem, bubble=2.5, pch=21, col=4, bg=4, main="")
abline(h=0, v=0, lty=3)
plot(envfit(vmod ~ N, varechem), col=1, cex=1, bg=8)
```

]

.pull-right[
]

---
# Significance tests

.pull-left[

- The strength of the fitted vector is given by the correlation $r$
  (or its square $r^2$) to the direction of the coordinates of the arrow head

- **Permutation tests:** Rows of environmental variables are shuffled
    into random order and $r^2$ is re-calculated

- The $P$-value is the probability that $r^2$ of random permutation is
  larger or equal to the observed value of fitted vector

- If this probability is low, say $P \le 0.05$, the variable is often
  regarded as significantly different from random

- Each variable is fitted independently, and other (correlated)
  variables do not influence its significance

]

.small[.pull-right[
```{r echo=FALSE, results=TRUE}
ef$vectors
```
]]

---
# Categorical Variables (Factors)

.pull-left[

- For one factor we can show its value for each point (and with tricks
  for two)

- The mean location or the **centroid** can be shown for several factors

- We also must assess the dispersion of points about the centroid
  &mdash; for this there are several graphical tools (next slide) for
  one factor in time

- Goodness of fit can be assessed as the proportion of total variance
  of points that can explained by the centroids

  - $r^2$ is the same statistic as for vectors, and for factors it
    gives the proportion of variance accounted for by the factor
    centroids



]

.pull-right[
```{r factor}
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
eff <- envfit(mod ~ Management, dune.env)
plot(mod, dis="si", type="n")
with(dune.env, points(mod, dis="si", pch=21, bg=viridis(4)[as.numeric(Management)]))
plot(eff, labels=levels(dune.env$Management), cex=1, bg=8)
legend("topright", levels(dune.env$Management), pch=21, pt.bg=viridis(4), title = "Management")
```
]
---
## Displaying Factors

```{r factorshow, fig.width=8, fig.height=5.6}
par(mar=c(3,4,0.8,1)+.1, mfrow=c(2,2))
palette(viridis(8))
plot(mod, dis="si", type="n", main="Spider")
ordispider(mod, dune.env$Management, col=viridis(4), label=TRUE)
with(dune.env, points(mod, dis="si", pch=21, bg=viridis(4)[as.numeric(Management)]))
plot(mod, dis="si", type="n", main="Convex Hull")
ordihull(mod, dune.env$Management, draw="poly", col=viridis(4), label=TRUE)
with(dune.env, points(mod, dis="si", pch=21, bg=viridis(4)[as.numeric(Management)]))
plot(mod, dis="si", type="n", main="SD Ellipse")
ordiellipse(mod, dune.env$Management, col=viridis(4), draw="poly", label=TRUE)
with(dune.env, points(mod, dis="si", pch=21, bg=viridis(4)[as.numeric(Management)]))
plot(mod, dis="si", type="n", main="Enclosing Ellipse")
ordiellipse(mod, dune.env$Management, kind="ehull", col=viridis(4), label=TRUE)
with(dune.env, points(mod, dis="si", pch=21, bg=viridis(4)[as.numeric(Management)]))
```

---
# Significance Tests for Factors

.pull-left[
.small[
```{r echo=FALSE, results=TRUE}
eff$factors
```
]

- **Confidence ellipse** is derived from Standard Error ellipse and
    shows the likely location of the centroid
    - SD ellipse shows the dispersion of points

- If confidence ellipses do not overlap, they are approximately
  different at the given level

]

.pull-right[
```{r confell}
palette(viridis(8))
par(mar=c(4,4,0.8,1)+.1)
plot(mod, dis="si", type="n", main="95% Confidence Ellipse")
ordiellipse(mod, dune.env$Management, kind="se", conf=0.95, draw="poly", col=viridis(4), label=TRUE, cex=1)
with(dune.env, points(mod, dis="si", pch=21, bg=viridis(4)[as.numeric(Management)]))
```
]

---
# Lessons

- Fitted vectors are the most concise method of displaying the effects
  of continuous environmental variables, **but...**

- They imply a linear trend, and you should always check that this is adequate

- The environmental variables are not usually parallel to axes, but
  they are oblique

- You should interpret the directions of the gradients (and the arrow
  points to the steepest gradient) instead of axes

- You should **never** **interpret** **axes**, but directions in the
  ordination space

- Significance tests can help in interpretation of results, but they
  should not be trusted blindly (or not at all): they are only correlations

---
class: inverse center middle

# PCoA: Principal Coordinates Analysis

Principal Coordinates Analysis (PCoA) uses dissimilarities among SUs
instead of rectangular data of SUs and variables. If dissimilarities
are Euclidean distances, then PCoA is equal to PCA. However, the
method can be used with other dissimilarities, and often these are
useful for community analysis. PCoA is also known as metric or classic
multidimensional scaling.
---
# Ordination of Distances

- PCA was based on variances and linear projections of species when
  deriving principal components from regressions

- The same information can be extracted from distances among SUs

- More precisely, these distances are known as Euclidean distances in
  the species space

- Why are they called **Euclidean**? &mdash; Because they **are**
  distances in Euclidean space!

- In species space, SUs are points on species axes that are at right
  angle to each others. The *difference* of species $j$ abundance for two
  SUs $i$ and $k$ is $y_{ij}-y_{kj}$ which is the *leg* or *cathetus* of a
  right-angled triangle, and the *squared* Euclidean distance of these two SUs
  over all *catheti* (species) is the squared *hypotenuse*
  $d_{ik}^2 = \sum_{j=1}^S (y_{ij}-y_{kj})^2$.

- It makes no sense to use Euclidean distances in PCoA, because PCA
  will give the same result more easily, but we can use other
  dissimilarities which are more useful in ecology

- These dissimilarities are handled similarly as Euclidean distances,
  but the results may be more useful (although there may be some
  geometrical glitches that we can usually ignore)

- All ordination methods we discuss are distance-based, and
  eigenvector methods (PCA, CA) are based on Euclidean distances or
  Euclidean distances of transformed data

---
## What is Wrong with Euclidean Distances?

- **They have no fixed upper limit,** but distances can vary among SUs
    that have nothing in common, and communities with high abundances
    appear more different than communities with low abundances

    - This can be cured with Euclidean distances of **transformed
      data**
    - For instance *Hellinger* or *Chord* distances have an
      upper limit if there are no shared species
      
    - If a distance can be expressed as a Euclidean distance of
      transformed data, it is better to use PCA with transformed data

- **They are based on squared differences** giving undue influence to
    exceptional high abundances

    - Instead of squared differences $(y_{ij}-y_{kj})^2$ we can use
      absolute differences $|y_{ij}-y_{kj}|$: *Manhattan* or
      *city-block* distances
      
    - Some dissimilarity measures combine both points: Bray-Curtis
      dissimilarity is based on absolute differences and is scaled to
      maximum value of one:
      $d_{ik} = \frac{\sum_j |y_{ij}-y_{kj}|}{\sum_j (y_{ij}+y_{kj})}$

- **Research interest dictates** to use a specific non-Euclidean
    dissimilarity index
---
# Example: beta Diversity

.pull-left[

- We want to study Whittaker's beta diversity
  $\beta = \gamma/\bar \alpha -1$, where $\gamma$ is the number of species in
  pooled communities, and $\bar \alpha$ is the average number of species in
  communities

- For two communities of $A$ and $B$ species with $J$ shared species, we have
  $\gamma = A+B-J$ and $\bar \alpha = (A+B)/2$ giving
$$\beta=\frac{A+B-J}{(A+B)/2}-1$$
$$~=\frac{2(A+B-J)}{A+B}-\frac{A+B}{A+B}$$
$$~=\frac{A+B-2J}{A+B}$$
   which is Sørensen dissimilarity

]

.pull-right[

```{r}
palette(viridis(8))
par(mar=c(4,4,0,1)+.1)
d <- designdist(varespec, "(A+B-2*J)/(A+B)", "binary")
dmod <- dbrda(d ~ 1)
plot(dmod)
```
Reindeer Pastures
]
---
# Example 2: Phylogenetic Distance

.pull-left[

- In normal dissimilarities, all species are regarded as equal

- If species are related, they would not contribute a full independent
  species to the dissimilarities

- Here we use Rao distance which is one of many phylogenetic
  dissimilarity indices (and they can equally well be used for related
  functional traits etc.)

- We put a limit: If species have diverged more than 65 Myr ago, they
  are regarded as completely independent

- PCoA is distance-based, and has no information on species, but these
  can be projected to the ordination afterwards similarly as species
  scores in PCA

]

.pull-right[
```{r}
palette(viridis(8))
par(mar=c(4,4,0,1)+.1)
plot(hclust(dune.phylodis), hang=-1, ylab="Time (Myr)", main="", sub="", xlab="")
abline(h=65, col=4)
```
Data for Tree: Zanne *et al* (2014), *Nature* **506,** 89&mdash;92.
]
---
# Example 2: Phylogenetic distance

```{r fig.width=11}
palette(viridis(8))
par(mar=c(4,4,0,1)+.1, mfrow=c(1,2))
m0 <- rda(dune)
pl <- plot(m0, type="n")
text(pl, "sites", pch=21, col=1, bg=1)
text(pl, "species", col=3, cex=0.8)
ordispider(m0, dune.taxon$Family, display="species", label=TRUE, col=3, cex=0.8)
d <- distrao(dune, dune.phylodis, dmax=65)
md <- dbrda(d ~ 1, sqrt.dist = TRUE)
sppscores(md) <- dune
pl <- plot(md, type="n")
text(pl, "sites", pch=21, col=1, bg=1)
text(pl, "species", col=3, cex=0.8)
ordispider(md, dune.taxon$Family, display="species", label=TRUE, cex=0.8)
```
---
class: inverse center middle

# NMDS: Nonmetric Multidimensional Scaling

PCA, CA & PCoA are eigenvector methods. They map community
dissimilarities linearly so that with the maximum number of dimensions
the dissimilarities are exactly reproduced, but some first dimensions
show as much as possible of the dissimilarities. NMDS is completely
different: it maps dissimilarities non-linearly onto low-dimensional
ordination. It assumes very little of data or response models and is
therefore more robust than eigenvector methods with their strict
parametric mapping.
---
# Eigenvector Methods: Linear Mapping

.pull-left[

- Eigenvector methods *decompose* observed dissimilarities

- With all possible axes, they reproduce exactly observed dissimilarities
- First axes show the largest possible amount of observed dissimilarities

- They map observed dissimilarities linearly so that they approach the
  original dissimilarities from below: each new axis *adds* to
  estimated dissimilarities

- Eigenvector methods may have their fixed way of defining observed
  dissimilarities: **PCA** uses Euclidean distances and **CA** uses
  $\chi$-distances (which are Euclidean distances of
  $\chi^2$-transformed data)

]

.pull-right[

```{r}
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
stressplot(cca(mite), l.col=2, p.col="white", bg=6, pch=21)
```
Oribatid mites: 2-dim CA of $\chi$-distances

]

---
## NMDS: Monotone Mapping

.pull-left[
- NMDS maps dissimilarities non-linearly onto low-dimensional ordination

- **Monotone regression**: Euclidean distances of points in the
    ordination space are **rank-order similar** to community
    dissimilarities
    - The **ordination space** is metric, but the **regression** is non-metric

- Rank-order similar: if *observed dissimilarity* $d_{a} < d_{b}$ then
  Euclidean *ordination distance* $\delta_{a} < \delta_{b}$
  
- The rank orders of observed dissimilarities cannot be exactly
  preserved by rank-orders of ordination distances in low-dimensional
  solutions, and this causes **stress**

- **Stress:** scatter of observed dissimilarities against
    expected monotone regression

]

.pull-right[
```{r}
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
mmod <- metaMDS(mite, trace=FALSE, parallel=2)
stressplot(mmod, l.col=2, p.col="white", bg=6, pch=21)
```
.small[Oribatid mites: NMDS in 2D]

]
---
## Stress and Iterations

$$\mathrm{stress} = \frac{\sum_{i \ne j}(\mathrm{monotone}(d_{ij})-\delta_{ij})^2}{\sum_{i \ne j}\delta_{ij}^2}\;,$$
where $d$ are the calculated dissimilarities between SUs $i$ and $j$,
$\mathrm{monotone}()$ is their monotone regression fit, and $\delta$ are the
Euclidean distances between points in the ordination

- In other words: stress is the spread of points with respect to
  monotone regression

- The ordination space is *metric* and Euclidean: the *non-metric*
  refers only to the regression used to transform observed
  dissimilarities (the step line in plots)

- NMDS is computationally much more demanding than eigenvector methods
  that are based on linear algebra (and the moden algorithms are
  really good in linear algebra)

- The solution is iterative and there is no guaranteed convergence,
  but we must repeat and judge the solutions for goodness and
  stability

- Stress is (in theory) in the range $0 \ldots 1$, but in practice
  stress $< 0.1$ is extremely good, stress close to $0$ hints to
  problems (like insufficient data or disjunctions), and random
  configurations in 2D have stress about $0.4$, leaving stress $0.2$
  as commonly very good

---
# Advantages of NMDS

- Eigenvector methods can give good ordination if the inherent
  ordination model is similar to the model species response to
  gradients

  - PCA model is linear and poor, but CA can cope with some regular
    unimodal response models and give good results

- NMDS does not assume any specific response model, but it can adjust
  to many models &mdash; even non-regular &mdash; and this makes it
  **robust**

- Robust: assumes little of data and tolerates violations of
  assumptions

- Can use ecological more meaningful dissimilarities than Euclidean
  distances and $\chi$-distances of PCA and CA
  
  - Shares this feature with PCoA (which still applies Euclidean
    mapping to these dissimilarities)

- Champion in tests with simulated data with non-regular unimodal
  species responses

---
## How Eigenvector Methods Differ from NMDS?

- **Origin:** shows the average (expected) community composition in
    eigenvector methods, but has **no** special meaning in NMDS

    - It is convenient to have the origin in the middle of the
      ordination in NMDS, but it means nothing: All that means is the
      configuration of points and distances between points &mdash; and
      these do not depend on the location of the origin

- **First axis** shows the major variation of the *data* (in terms of
    inertia, either variance of $\chi^2$) in eigenvector methods, but
    has **no** special meaning in NMDS

    - It is convenient to rotate NMDS space so that the first axis
      shows the major variation of the *ordination*, but it means
      nothing: All that means is the configuration of points and
      distances between points &mdash; and these do not depend on the
      directions of the axes

    - For convenience NMDS axes are often rotated to Principal
      Components, but they can be rotated to be parallel to
      environmental variables or any other direction

- What is in common is that NMDS and Eigenvector methods (PCA, CA, PCoA)
  have **metric ordination space** that can be interpreted in the same way
  for the configuration of points and using environmental variables
---
# These NMDS Ordinations are Identical

```{r fig.width=11}
palette(viridis(8))
par(mar=c(4,4,0,1)+.1, mfrow=c(1,2))
plot(mmod, type="n")
text(mmod, dis="si", col=1, cex=0.8)
text(mmod, dis="sp", col=5, cex=0.8, xpd=TRUE)
plot(envfit(mmod ~ SubsDens + WatrCont + Shrub, mite.env), col=3, bg=8, xpd=TRUE)
mrot <- MDSrotate(mmod, mite.env$WatrCont)
plot(mrot, type="n")
text(mrot, dis="si", col=1, cex=0.8)
text(mrot, dis="sp", col=5, cex=0.8, xpd=TRUE)
plot(envfit(mrot ~ SubsDens + WatrCont + Shrub, mite.env), col=3, bg=8, xpd=TRUE)

```
---
## Example: Abernethy Pollen Profile

```{r abergrid, fig.width=8,fig.height=5.5, results=FALSE}
palette(viridis(8))
par(mar=c(4,4,0.7,1)+.1, mfrow=c(2,2))
#x <- rbinom(prod(dim(mtfit)), 1, mtfit)
#dim(x) <- dim(mtfit)
x <- abernethy[,1:36]
m1 <- rda(decostand(x, "hell"), scale=FALSE)
pl <- plot(m1, dis="si", scaling="si", type="n", main="PCA: Hellinger")
tmp <- ordisurf(pl, abernethy$Age, add=T, col=4)
grp <- rep(1, nrow(x)) # for ordiarrows
ordiarrows(pl, grp, col=7)
points(pl, dis="si", pch=16, col=2)
m2 <- cca(x)
pl <- plot(m2, dis="si", scaling="si", type="n", main="CA: Chi")
tmp <- ordisurf(pl, abernethy$Age, add=T, col=4)
ordiarrows(pl, grp, col=7)
points(pl, dis="si", pch=16, col=2)
m3 <- dbrda(vegdist(wisconsin(x))~1)
pl <- plot(m3, dis="si", scaling="si", type="n", main="PCoA: Bray-Curtis / Wisconsin")
tmp <- ordisurf(pl, abernethy$Age, add=T, col=4)
ordiarrows(pl, grp, col=7)
points(pl, dis="si", pch=16, col=2)
m4 <- metaMDS(vegdist(wisconsin(x)), trace=FALSE)
m4 <- MDSrotate(m4, abernethy$Age)
pl <- plot(m4, dis="si", scaling="si", type="n", main="NMDS: Bray-Curtis / Wisconsin")
tmp <- ordisurf(pl, abernethy$Age, add=T, col=4)
ordiarrows(pl, grp, col=7)
points(pl, "sites", pch=16, col=2)
```
---
## Stress Plot Indicates NMDS Does Best

```{r aberstress, fig.width=8,fig.height=5.5, results=FALSE}
par(mar=c(4,4,0.7,1)+.1, mfrow=c(2,2))
palette(viridis(8))
stressplot(m1, l.col=2, p.col="white",bg=6,pch=21, main="PCA: Hellinger")
stressplot(m2, l.col=2, p.col="white",bg=6,pch=21, main="CA: Chi")
stressplot(m3, l.col=2, p.col="white",bg=6, pch=21,main="PCoA: Bray-Curtis / Wisconsin")
stressplot(m4, l.col=2, p.col="white",bg=6,pch=21, main="NMDS: Bray-Curtis / Wisconsin")
```
---
# Is NMDS Credible?

.pull-left[

- NMDS is the only method that does not produce a curve &mdash; but is
  this credible?

- There is a cycle in SU sequence in the oldest times. Species scores
  indicate that the cycle is *Rumex* &mdash; *Anthyllis vulneraria*
  &mdash; Graminoids &mdash; *Artemisia* and then going to the linear
  succession *Juniperus communis* &mdash; *Betula* &mdash; *Pinus
  sylvestris* &mdash; *Alnus glutinosa*

- This looks like a credible cycle of pioneer communities (*Rumex*)
  and tundra steppes (*Artemisia*) before going to secular heath and
  forest succession

- All other methods had a curve that hid the pioneer cycle

]

.pull-right[

```{r abernmds}
par(mar=c(4,4,0,0)+.1)
palette(viridis(8))
sppscores(m4) <- x
pl <- plot(m4, type="n")
text(pl, "sites", labels=abernethy$Age, cex=0.7)
ordiarrows(pl, rep("a",nrow(abernethy)), col=7)
ordilabel(pl, dis="sp", cex=0.7, labels=make.cepnames(colnames(x)), priority=colSums(x), col=3)
```

]

---
class: inverse center middle

# Constrained Ordination

Constrained ordination is also known as canonical ordination. In
unconstrained ordination we used only community data to find its main
structure, and interpreted the results via fitting environmental
variables to the ordination. In constrained ordination we derive the
result directly through environmental variables (constraints) and find
only that structure in the communities that can be explained with
constraints. The results explain a smaller propotion of total
variation than unconstrained ordination, but they are more strongly
interpretable with the constraints. Constrained ordination is allows
for statistical testing of the constraints, and it is most useful in
the analysis of designed experiments.

---
# Constrained Ordination

- In *unconstrained* ordination (PCA, CA, PCoA) we explained species
  with species and found the best possible linear predictors of
  observed values

- In *constrained* ordination we explain species with external
  variables (such as environement)

- This gives poorer results than unconstrained ordination, but more
  easily interpretable because we constrain the results with
  variabales we use in interpretation

- In unconstrained ordination we first find the ordination and then
  see how it can be explained

- In constrained ordinatin we build the ordination so that it can be
  explained with the environmental variables (constraints)

---
# From Regression to Constrained Ordination

- Constrained Ordination: Find the *linear combination* of
  **constraints** that best explains all species

- These two alternative strategies lead to the same result:

.pull-left[

**Method 1**

- Use the linear combination of constraints as the explonatory
  variable, and find such a linear combination that has smallest
  residual variation for all species

- We use the same linear combination for all species and that is the
  first constrained ordination axis

- This directly gives *regression coefficients* for environmental
  variables and the first constrained axis

]

.pull-right[

**Method 2**

- Fit a linear regression separately for all species using all
  constraints as explanatory variables

- Each species has different regression models

- Ordinate the fitted values of species as in unconstrained ordination

- This directly gives *species scores* and the first constrained axis

]

---
# Method 1: LC of Constraints

```{r rdamethod1, fig.width=11}
par(mar=c(4,4,1,1)+.1, mfrow=c(1,3))
palette(viridis(8))
cmod <- rda(bott0 ~Al + P, varechem)
rda1 <- cmod$CCA$u[,1]
koef <- coef(cmod)[,1]
koef <- paste(formatC(koef, digits=3, flag="+"), names(koef), sep="*", collapse="")
ylim <- range(bott0)
for(i in 1:3) {
   plot(rda1, bott0[,i], ylim=ylim, xlab=koef, ylab=colnames(bott0)[i], type="n", cex.lab=1.6)
   panel.lm(rda1, bott0[,i], pch=16, col=2, col.smooth=4)
}
```

---
# Method 2: Ordination of Fitted Values

```{r rdamethod2a, fig.width=8, fig.height=5.5}
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
pairs(fitted(cmod), pch=16, col=4, ylim=range(fitted(cmod)), panel=panel.lm)
```

---
# Method 2: Ordination of Fitted Values

```{r rdamethod2b, fig.width=11}
par(mar=c(4,4,1,1)+.1, mfrow=c(1,3))
palette(viridis(8))
rda1 <- cmod$CCA$u[,1]
koef <- cmod$CCA$v[,1]
koef <- paste(formatC(koef, digits=2, flag="+"), names(koef), sep="*", collapse="")
fv <- fitted(cmod)
ylim <- range(fv)
for(i in 1:3) {
   plot(rda1, fv[,i], ylim=ylim, xlab=koef, ylab=colnames(fv)[i], type="n", cex.lab=1.5, cex=1.2)
   panel.lm(rda1, fv[,i], pch=16, col=2, col.smooth=4)
}
```
---
# What Was Explained?

- **Method 1** explained *total* variance

  - It was `r round(cmod$tot.chi, 1)`, the first axis explained
    $\lambda_1=$ `r round(cmod$CCA$eig[1], 1)`, and all constrained
    axes explained $\Lambda_k=$ `r round(cmod$CCA$tot.chi, 1)`

- **Method 2** explained the variance of *fitted values*

  - It was $\Lambda_k=$ `r round(cmod$CCA$tot.chi, 1)`, and the first
    axis explained $\lambda_1=$ `r round(cmod$CCA$eig[1], 1)`

- We had two constraints (Al & P) and therefore we had two constrained
  axes: the **rank** of constraints was 2

- Both methods gave the same two constrained axes and the same
  eigevalues (explained variance)

- The sum of first two eigenvalues was
  `r round(sum(rda(bott0)$CA$eig[1:2]), 1)` which is more than
  constrained $\Lambda_k$

- Unconstrained PC axes are the best possible linear predictors and
  anything else must be worse

- Constrained ordination should have *clearly* lower eigenvalues than
  unconstrained &mdash; or constraints did not have any effect!

---
# Flavours of Constrained Ordination

- Constrained ordination is an extension of eigenvector ordination

- **Redundancy Analysis (RDA):** extends PCA

  - Explains variance, shows Euclidean distances

  - PCA was poor for community data, but RDA ordinates fitted values
    &mdash; and these are linear &mdash; and RDA works OK with
    communities

  - It is still best to use correlations or Hellinger distances to
    tune down superdominant species

- **Constrained or Canonical Correspondence Analysis (CCA):** extends CA

  - Explains $\chi^2$, shows $\chi$-distances
  
  - The most popular approach (but this depends on your cultural
    background: Francophones prefer RDA)

- **Distance-based Redundancy Analysis (dbRDA):** extends PCoA

  - Explains any squared dissimilarities, shows any dissimilarities
  
  - If you are using dissimilarities elsewhere, it is consistent to
    use them in constrained ordination

---
# The Scores

- **Species scores**

  - These are also regression coefficients that derive the constrained
    axis from species data

- Two kind of **SU scores**

  - **Linear combination (LC) scores** which are the linear
      combinations of *constraints*

  - Together with the species scores, the LC scores find the *fitted*
    values of each species (of method 2)

  - **Weighted Averages (WA) scores** which are derived from the
      species scores and observed values of the input community

  - Together with the species scores, the WA scores estimate the
    *observed* values of each species

- **Biplot scores** for constraints

  - These are correlations with the constraints and the LC scores for SUs

  - They are found separately for each constraining variable, and
    other constraining variables do not influence their values

---
# Interpretation of Biplot Scores

.pull-left[

- LC scores really are linear combinations of constraints, and when
  projected to biplot arrows, they fall on the values of constraints

- When WA scores are projected onto biplot arrows, they give the
  prediction of environmental conditions from species data:
  bioindication, reconstruction of environment

- All biplot arrows have correlation $r=1$ over all constrained axis:
  the LC scores are made of them, and therefore the correlation is
  perfect

- We can also have *regression scores* which together with all other
  constraints define the fitted values of community, but they are
  difficult to interpret and never used in graphics

]

.pull-right[

```{r rdabp, results=FALSE}
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
mrda <- rda(varespec ~ Al + P, varechem)
plot(mrda, dis=c("lc","bp"), type="n")
points(mrda, dis="lc", pch=21, bg=4, col=1)
text(mrda, dis="bp", col=1, lwd=2)
ordisurf(mrda ~ Al, varechem, add=T, knots=1, col=5, display="lc")
ordisurf(mrda ~ P, varechem, add=T, knots=1,col=5, display="lc")
```
Complete Reindeer Pasture Data
]

---
# Factor Constraits

.pull-left[

- Factors (categorical variables) are coded as **contrasts**

- For instance, a factor with four classes is coded by three indicator
  variables that express the difference against the corner-point case

- Ordered factors can be expressed as polynomial contrasts that
  describe the linear, quadratic, cubic *etc* effect of the variable

- These contrasts are used as ordinary continuous variables in the analysis

- Contrasts can be coded in many alternative way, but all give the
  same fitted results and the same ordination

]

.pull-right[
```{r echo=FALSE}
data(dune.env)
cat("*Dunes -- Management:\n")
contrasts(dune.env$Management)
cat("\n*Dunes -- Moisture (ordered factor):\n")
contrasts(dune.env$Moisture)
```
]

---
# Ordination Display of Factors

.pull-left[

- Factors are displayed by the centroids of their levels

- The ordered factors can be described by biplot arrows for their
  polynomial contrasts in addition to the centroid

- The types of contrasts do not influence the centroids, and we rarely
  need to know the exact type of contrasts used

  - Contrasts are internal to the program

]

.pull-right[
```{r dunefact}
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
m <- rda(dune ~ Moisture + Management, dune.env)
plot(m, dis=c("cn","wa"), type="n", scaling="si")
points(m, dis="wa", pch=21, col=1, bg=5, scaling="si")
text(m, dis="cn", col=2, xpd=T, scaling="si")

```
]

---
# LC or WA Scores?

.pull-left[

- LC scores really are linear combinations of constraings: and this
  means that they really are defined by constraints only

- Equal constraints means equal LC scores &mdash; and the species
  composition has no influence

- For a single factor constraint, LC scores with the same factor level
  will fall into same point

- WA scores show scatter with respect to LC scores: they tell how well
  species composition can *predict* the constraints

- LC are completely defined also for several factors and continuous
  variables, but with larger number of constraints this is not
  immediately visible

]

.pull-right[
```{r lcwa}
par(mar=c(4,4,0,1)+.1)
palette(viridis(4))
m <- rda(dune ~ Management, dune.env)
plot(m, dis=c("lc","wa"), type="n", scaling="si")
ordispider(m, scaling="si", col=3)
points(m, dis="wa", pch=21, col=1, bg=dune.env$Management, scaling="si")
points(m, dis="lc", pch=21, col=1, bg=dune.env$Management, cex=3, scaling="si")
legend("bottomleft", levels(dune.env$Management), pch=21, col=1, pt.bg=1:4)
legend("topleft", c("WA","LC"), pch=21, col=1, pt.bg=1, pt.cex=c(1,3))
```
Dune Meadows constrained by Management
]

---
# WA Scores & Prediction of Environment

.pull-left[

- LC scores *are* the environment, but WA scores are based on
  community composition

- WA scores are as similar to LC scores as possible: they predict the
  environment as well as the community composition can (with linear
  mathematics applied in ordination)

- This is known as biondication, calibration or reconstruction of
  environment in the various field of Ecology, Environmental
  Sciences and Palaeoecology

- We should always use cross-validation to assess the goodness of
  prediction: test performance of the method in another subset of the
  data than was used in constrained ordination

]

.pull-right[
```{r rdacalib, fig.height=4.5}
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
mmite <- rda(decostand(mite, "hell") ~ WatrCont + SubsDens + Topo + Shrub,
   data=mite.env)
cal <- calibrate(mmite)[,"WatrCont"]
plot(cal ~ WatrCont, mite.env, xlab="Observed WatrCont", ylab="WatrCont Predicted from WA scores", pch=21, col="white", bg=4, cex=1.2)
abline(0, 1, lwd=3, col=1)
legend("topleft", "Identity", lty=1, lwd=3, col=1, bty="n")
```
.small[`r deparse(mmite$call)`]
]

---
class: inverse center middle

# Permutation Tests

Constrained Ordination is based on regression with external variables,
and this allows for statistical analysis of constraints. The response
data are multivariate, correlated and non-Normal, and we cannot use
normal paametric tests, but we must use permutation tests.

---
# Permutation Tests

- The goodness of ordination solution is analysed using similar
  $F$-statistic as in ordinary ANOVA
$$F = \frac{\Lambda_k/p}{\Lambda_r/(n-p-1)}$$
  where $\Lambda_k$ is the constrained inertia explained by $p$ constrained
  axes, $\Lambda_r$ is the inertia of residuals, and $n$ is the number of
  SUs

- The total inertia $\Lambda$ (variance, $\chi^2$, squared
  dissimilarities) is decomposed as $\Lambda = \Lambda_k + \Lambda_r$

- In permutation test the response is shuffled into random order, the
  ordination is repeated and the $F$ is evaluated

- This gives us a great number of permutation $F$-values of randomized data

- The observed $F$ is put among the permutation $F$, and all $F$ are
  ordered by maginuted

- The $P$-value is the proportional rank of observed $F$: this gives
  the probability to have similar or better $F$ in random data

- If we have 999 permutations and we add the observed $F$ to these,
  the proportial ranks are for 1000 values, and the lowest possible
  $P=$  0.001

---
# Overall Test

.pull-left[
.small[
```{r echo=FALSE}
mano <- cca(dune ~ A1 + Management + Moisture, dune.env)
ano <- anova(mano)
ano
```
]

- Management has 4 factor levels and uses 3 contrasts, Moisture is a
  4-level ordered factor and uses 3 contrasts, and A1 is continuous
  variable, and together these use 7 degrees of freedom, and the total
  inertia is `r round(mano$tot.chi, 3)` giving

$$F = \frac{`r round(mano$CCA$tot.chi, 3)`/`r mano$CCA$rank`}{`r round(mano$CA$tot.chi, 3)`/(`r nobs(mano)`-`r mano$CCA$rank` -1)}$$


]

.pull-right[
```{r anova, echo=FALSE}
par(mar=c(4,4,0,1)+.1)
palette(viridis(8))
# I hate trellis
lstrip <- trellis.par.get("strip.background")
lstrip$alpha <- 0.5
lstrip$col <- rev(viridis(8))
trellis.par.set("strip.background", lstrip)
densityplot(permustats(ano), pch=16, cex=0.6, col=1)
```
]
---
class: inverse center bottom

# The End
--

## Thank You

---
.small[
```{r end, echo=FALSE}
options(width=132)
sessionInfo()
``` 
]